{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00391b20-16e0-48f5-bd43-9d533d53b97c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r '/Workspace/Users/chrismarshall.wi@icloud.com/dbxProjects/Alpha Factors/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790c374d-598b-46e5-a8c7-dc0ff6882b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import pandas as pd\n",
    "sys.path.append('/Workspace/Users/chrismarshall.wi@icloud.com/dbxProjects/Alpha Factors')\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "178bb702-0ecf-4641-bab5-744d9d45b613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394e5f3d-bc94-422b-bbf2-11c353250bf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open('sql/fact_price_daily_backtest.sql', 'r') as file:\n",
    "    qry = file.read()\n",
    "\n",
    "data = spark.sql(qry).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747c63c4-9cf1-498a-8911-05c7e939dca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utilities import Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ae412b-609d-4a10-81bd-80b5d1305c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = Factor(data=data)\n",
    "data.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd35a7f3-34fb-4375-b464-1f29826d43ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def backtest_mean_reversion(df:pd.DataFrame, moving_average_window:float,start_date:str, end_date:str):\n",
    "    '''\n",
    "    Backtest Mean Reversion strategy using a moving average window (typically 200 days) and start/end date\n",
    "    '''\n",
    "    #First assert that there are 3 columns named symbol, date_value and close\n",
    "    required_columns = ['symbol','date_value','close']\n",
    "    if list(df.columns)!=required_columns:\n",
    "        raise ValueError(f'The columns must be {required_columns}')\n",
    "\n",
    "    #Second assert that the date_value column is pd.to_datetime()\n",
    "    df['date_value'] = pd.to_datetime(df['date_value'])\n",
    "\n",
    "    #Sort Values before applying moving average\n",
    "    df = df.sort_values(['symbol','date_value']).copy()\n",
    "\n",
    "    #Fourth calculate the moving average based on what was passed\n",
    "    df[f'{moving_average_window}_moving_average'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(moving_average_window).mean())\n",
    "\n",
    "    #Compute Previous values\n",
    "    df['previous_close'] = df.groupby('symbol')['close'].shift(1)\n",
    "    df[f'previous_{moving_average_window}_moving_average'] = df.groupby('symbol')[f'{moving_average_window}_moving_average'].shift(1)\n",
    "\n",
    "    #Last Step: filter the dataframe to the start/end dates (add other filters here in the future to create df_filtered)\n",
    "    df_filtered = df[(df['date_value'] >= start_date) & (df['date_value'] <= end_date)]\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170dce67-3f0a-4a71-bcd9-f743ad55dfb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=backtest_mean_reversion(df=data,moving_average_window=200,start_date='2010-01-01',end_date='2010-12-31')\n",
    "df_spark=spark.createDataFrame(df)\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteschema\", True).saveAsTable(\"operations.finance.fact_price_daily_moving_average\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fact_price_daily_backtest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
